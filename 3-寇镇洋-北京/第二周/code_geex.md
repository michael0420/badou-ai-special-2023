[//]: # "# AI tech study summary"

# week 02

## 数学基础学习

<!-- 向量、线性变换、矩阵、导数&偏导数、梯度、概率学基础和熵的数学基础 -->

### 向量

基本介绍：

- 向量是数学中的基本概念，在机器学习中，向量是模为 1 的复数，即向量是复数空间 C^n 中的一个点。

向量的基本运算：

- 向量加法：$v1 + v2$
- 向量减法：$v1 - v2$
- 向量点乘：$v1 \cdot v2$
- 向量叉乘：$v1 \times v2$
- 向量模：$|v|$
- 向量模的平方：$(|v|)^2$
- 向量除法：$v1 / v2$
- 向量逆：$v^{-1}$
- 向量转置：$v^T$
- 向量范数：$||v||$
- 向量范数的平方：$||v||^2$
- 向量范数的平方根：$\sqrt{||v||^2}$

### 线性变换

基本介绍：

- 线性变换是线性算子，线性变换的输入输出都是向量。

基本形式：

- 线性变换的矩阵形式：$T(v) = Av$
- 线性变换的向量形式：$T(v) = v^T A$

$T(v + w) = T(v) + T(w)$
$T(cv) = cT(v)$

### 矩阵

基本介绍：

- 矩阵是线性代数中的基本概念，在机器学习中，矩阵是模为 1 的复数，即矩阵是复数空间 C^n×n 中的一个点。

#### 单位矩阵

单位矩阵是一个对角线为 1，其余为 0 的方阵。

#### 逆矩阵

基本介绍：

- 矩阵的逆矩阵，是矩阵的逆，即$A^{-1}A = AA^{-1} = I$，I 是单位矩阵。

#### 奇异矩阵

基本介绍：当矩阵没有逆矩阵时，矩阵称为奇异矩阵。

当且仅当矩阵的行列式为 0 时，矩阵称为奇异矩阵。

$$
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$

$$
A^{-1} = \cfrac{1}{|A|}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
=
\frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
$$

当 ad-bc = 0 时，|A|没有定义，$A^{-1}$不存在，此时 A 是奇异矩阵。

#### 转置矩阵

基本介绍：

- 矩阵的转置矩阵，是矩阵的转置，即行列呼唤，用$A^T$表示 A 的转置矩阵。

特性：

- $(A^T)^T = A$
- $(AB)^T = B^TA^T$

#### 对称矩阵

基本介绍：

- 矩阵的对称矩阵，是矩阵的转置等于自身，即$A^T = A$。

一个矩阵转置和这个矩阵的乘积就是一个对称矩阵。

#### 矩阵乘法

基本介绍：

- 矩阵的乘法，是矩阵的乘积，即$A \cdot B$。

#### 矩阵的迹

基本介绍：

- 矩阵的迹，是矩阵对角线元素之和，即$\mathrm{tr}(A) = \sum_{i=1}^n a_{ii}$。

#### 矩阵的秩

基本介绍：

- 矩阵的秩，是矩阵的行和列中，非零向量的个数，即$\mathrm{rank}(A) = \mathrm{rank}(A^T)$。

#### 欧氏变换

基本介绍：

- 欧氏变换，是线性变换，即$T(v) = \lambda v$。

欧氏变换由两部分组成：

- 缩放：$T(v) = \lambda v$
- 旋转：$T(v) = \lambda v^T$

## 机器学习基础学习

<!-- 线性回归、逻辑回归、决策树、随机森林、KNN、朴素贝叶斯、SVM、神经网络、卷积神经网络、循环神经网络、深度学习、强化学习 -->

### 线性回归

基本介绍：

- 线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互关系的模型。

基本公式：

$$
y = w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n
$$

### 逻辑回归

基本介绍：

- 逻辑回归是用来计算一个事件发生的概率。

基本公式：

$$
P(y=1|x) = \frac{1}{1+e^{-z}}
$$

### 决策树

基本介绍：

- 决策树是一种基本的分类与回归方法。

基本公式：

$$
P(y=1|x) = \frac{1}{1+e^{-z}}
$$

### 随机森林

基本介绍：

- 随机森林是机器学习中的集成算法，由多个决策树组成。

基本公式：

$$
P(y=1|x) = \frac{1}{1+e^{-z}}
$$

### KNN

基本介绍：

- KNN（K-Nearest Neighbors）是一种监督学习的方法，是分类算法中的一种。

基本公式：

$$
J(w, b) = \frac{1}{2} \sum_{i=1}^n \left( y^{(i)} - h_\theta(x^{(i)}) \right)^2
$$

### 朴素贝叶斯

基本介绍：

- 朴素贝叶斯是一类基于贝叶斯定理基础上的统计学方法。

### 支持向量机

基本介绍：

- 支持向量机（Support Vector Machine, SVM）是一类按监督学习方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其分类原理是使用一种称为核技巧（kernel trick）的技术把数据转换为高维空间来进行分类。

基本公式：
$
\begin{aligned}
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{s.t. } y^{(i)}( \mathbf{w}^T \phi(\mathbf{x}^{(i)}) + b) \geq 1, \quad i = 1, \ldots, m
\end{aligned}
$

### 神经网络

基本介绍：

- 神经网络是一种模仿生物神经网络的结构和功能的数学模型。

基本公式：

$$
\begin{aligned}
z_i &= \sum_{j=1}^{n} w_{ij} x_j + b_i \\
a_i &= \phi(z_i)
\end{aligned}
$$

## 深度学习基础学习

<!-- 卷积神经网络、循环神经网络、生成对抗网络、深度强化学习、迁移学习 -->

### 卷积神经网络

基本介绍：

- 卷积神经网络（Convolutional Neural Networks, CNN）是一种前馈神经网络，它具有多层卷积 channel 和 Pooling 层，它的设计目的是通过用卷积层对图像进行多层处理来提取图像特征。

### 循环神经网络

基本介绍：

- 循环神经网络（Recurrent Neural Networks, RNN）是一种特殊的神经网络，它具有循环层，循环层中的神经元可以处理序列数据。

### 生成对抗网络

基本介绍：

- 生成对抗网络（Generative Adversarial Networks, GAN）是一种深度学习模型，它通常用于生成图像、视频等。

### 深度强化学习

基本介绍：

- 深度强化学习（Deep Reinforcement Learning, DRL）是一种基于深度学习的强化学习算法，它通常用于解决复杂决策问题。

### 迁移学习

基本介绍：

- 迁移学习（Transfer Learning）是一种深度学习技术，它将预训练的模型应用于新的任务。

## 自然语言处理基础学习

<!-- 词向量、语言模型、Seq2Seq、Attention、Transformer、BERT、GPT、RoBERTa、Megatron-LM、GPT-3 -->

### 自然语言处理基础

基本介绍：

- 自然语言处理（Natural Language Processing, NLP）是一门计算机科学领域，它研究如何对人类语言进行计算机处理。

### 词向量

基本介绍：

- 词向量（Word Vector）是一种用于表示自然语言中单词的向量表示方法。

### 语言模型

基本介绍：

- 语言模型（Language Model）是一种用于计算一个句子概率的模型。

### Seq2Seq

基本介绍：

- 序列到序列（Sequence to Sequence, Seq2Seq）是一种用于自然语言处理的深度学习模型。

### Attention

基本介绍：

- 注意力（Attention）是一种用于计算序列中每个元素重要性的方法。

### Transformer

基本介绍：

- transformer（Transformer）是一种用于自然语言处理的深度学习模型。

### BERT

基本介绍：

- BERT（Bidirectional Encoder Representations from Transformers）是一种用于自然语言处理的深度学习模型。

### GPT

基本介绍：

<!-- GPT产生背景、GPT版本迭代故事、GPT原理 -->

GPT（Generative Pre-trained Transformer）是一种用于自然语言处理的深度学习模型。

GPT 版本迭代故事：

- GPT-1：2018 年，OpenAI 发布 GPT-1，其性能远超当时已有的语言模型
- GPT-2：2019 年，OpenAI 发布 GPT-2，其性能远超当时已有的语言模型
- GPT-3：2020 年，OpenAI 发布 GPT-3，其性能远超当时已有的语言模型

GPT 原理：

- GPT-1：基于 Transformer 的编码器-解码器架构
- GPT-2：基于 Transformer 的编码器-解码器架构
- GPT-3：基于 Transformer 的编码器-解码器架构

### RoBERTa

基本介绍：

- RoBERTa（RoBERTa: A Robustly Optimized BERT Pretraining Approach）是一种用于自然语言处理的深度学习模型。

### Megatron-LM

基本介绍：

- Megatron-LM（Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism）是一种用于自然语言处理的深度学习模型。

### GPT-3

基本介绍：

- GPT-3（GPT-3: Language-Generation Tool）

## 计算机视觉基础学习

<!-- 图像分类、目标检测、实例分割、语义分割、超分辨率、视频理解、生成式对抗网络 -->

### 图像分类

<!-- 基本介绍、产生背景、应用场景、原理说明 -->

> 基本介绍

图像分类（Image Classification）是计算机视觉中的一项基本任务，用于对图像进行分类。

> 产生背景

图像分类是计算机视觉中的一项基本任务，用于对图像进行分类。

> 应用场景

图像分类在计算机视觉中具有广泛的应用场景，例如：

- 图像检索
- 图像去重

> 原理说明

图像分类的原理是利用卷积神经网络（Convolutional Neural Network，CNN）对图像进行特征提取，然后利用分类器（Classifier）对图像进行分类。

### 目标检测

<!-- 基本介绍、产生背景、应用场景、原理说明 -->

> 基本介绍

目标检测（Object Detection）是计算机视觉中的一项基本任务，用于对图像中的目标进行检测和定位。

> 产生背景

目标检测是计算机视觉中的一项基本任务，用于对图像中的目标进行检测和定位。

> 应用场景

目标检测在计算机视觉中具有广泛的应用场景，例如：

- 图像检索
- 图像去重

> 原理说明

目标检测的原理是利用卷积神经网络（Convolutional Neural Network，CNN）对图像进行特征提取，然后利用分类器（Classifier）对图像中的目标进行检测和定位。

### 实例分割

<!-- 基本介绍、产生背景、应用场景、原理说明 -->

> 基本介绍

实例分割（Instance Segmentation）是计算机视觉中的一项基本任务，用于对图像中的目标进行分割和定位。

> 产生背景

实例分割是计算机视觉中的一项基本任务，用于对图像中的目标进行分割和定位。

> 应用场景

实例分割在计算机视觉中具有广泛的应用场景，例如：

- 图像检索
- 图像去重

> 原理说明

实例分割的原理是利用卷积神经网络（Convolutional Neural Network，CNN）对图像进行特征提取，然后利用分割器（Segmentor）对图像中的目标进行分割和定位。

### 语义分割

<!-- 基本介绍、产生背景、类型、应用场景 -->

> 基本介绍

语义分割（Semantic Segmentation）是计算机视觉中的一项基本任务，用于识别图像中存在的内容以及位置

> 产生背景

语义分割是计算机视觉中的一项基本任务，用于识别图像中存在的内容以及位置。

> 类型

语义分割可以分为像素级语义分割和实例级语义分割。

- 像素级语义分割：每个像素点都被分配一个标签，用于表示该像素点的类别。
- 实例级语义分割：每个实例都被分配一个标签，用于表示该实例的类别。

> 应用场景

语义分割在计算机视觉中具有广泛的应用场景，例如：

- 地质检测——土地使用
- 道路检测——道路类型
- 建筑物检测——建筑物类型
- 车辆检测——车辆类型
- 行人检测——行人类型

## 推荐系统基础学习

<!-- 推荐系统基础、矩阵分解、隐语义模型、协同过滤、深度学习推荐、深度学习推荐系统 -->

## 数据挖掘基础学习

<!-- 数据挖掘基础、关联规则、聚类、分类、回归、异常检测、时间序列、推荐系统、推荐系统、推荐系统 -->

## 数据科学基础学习

<!-- 数据科学基础、数据仓库、数据建模、数据可视化、数据挖掘、数据科学 -->

## 数据科学项目学习

<!-- 数据科学项目、数据科学项目 -->
